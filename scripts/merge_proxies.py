# -*- coding: utf-8 -*-
import yaml
import glob
import logging
import re
import argparse

# --- é…ç½®æ—¥å¿—è®°å½• ---
# è®¾ç½®æ—¥å¿—çš„æ ¼å¼å’Œçº§åˆ«ï¼Œæ–¹ä¾¿è°ƒè¯•å’Œè¿½è¸ªè„šæœ¬æ‰§è¡Œæƒ…å†µ
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler()  # è¾“å‡ºåˆ°æ§åˆ¶å°
    ]
)

# --- å¸¸é‡å®šä¹‰ ---
# å®šä¹‰ä¸åŒåœ°åŒºçš„æ­£åˆ™è¡¨è¾¾å¼è¿‡æ»¤å™¨ï¼Œç”¨äºæ ¹æ®èŠ‚ç‚¹åç§°ç­›é€‰ç‰¹å®šåœ°åŒºçš„ä»£ç†
FILTER_PATTERNS = {
    'hk': re.compile(
        r'\b(HK|Hong[\s_-]?Kong|HKG|HGC)\b(?!-?(check|fail))|é¦™æ¸¯|æ¸¯|ğŸ‡­ğŸ‡°',
        flags=re.IGNORECASE
    ),
    'us': re.compile(
        r'\b(us|usa|america|united[\s-]?states)\b(?!-?(check|fail))|ç¾|ğŸ‡ºğŸ‡¸',
        flags=re.IGNORECASE
    ),
    'jp': re.compile(
        r'\b(jp|japan|tokyo|tyo|osaka|nippon)\b(?!-?(check|fail))|æ—¥æœ¬|æ—¥|ğŸ‡¯ğŸ‡µ',
        flags=re.IGNORECASE
    ),
    'uk': re.compile(
        r'\b(uk|england|britain|united[\s-]?kingdom)\b(?!-?(check|fail))|è‹±å›½|è‹±|ğŸ‡¬ğŸ‡§',
        flags=re.IGNORECASE
    ),
    'sg': re.compile(
        r'\b(sg|singapore|sin)\b(?!-?(check|fail))|æ–°åŠ å¡|æ–°|ğŸ‡¸ğŸ‡¬',
        flags=re.IGNORECASE
    ),
}

# å®šä¹‰ä¸å¸Œæœ›åŒ…å«çš„ä»£ç†åç§°å…³é”®è¯ï¼ˆé»‘åå•ï¼‰ï¼Œæ­¤è§„åˆ™å¯¹æ‰€æœ‰åˆå¹¶ä»»åŠ¡ç”Ÿæ•ˆ
BLACKLIST_KEYWORDS = [
    'ç”µæŠ¥', 'æ—¥æœŸ', 'å…è´¹', 'å…³æ³¨'
]

def merge_proxies(proxies_dir, output_file, name_filter=None):
    """
    åˆå¹¶æŒ‡å®šç›®å½•ä¸‹çš„æ‰€æœ‰ä»£ç†é…ç½®æ–‡ä»¶ã€‚

    :param proxies_dir: å­˜æ”¾ä»£ç†é…ç½®æ–‡ä»¶çš„ç›®å½•è·¯å¾„ (ä¾‹å¦‚: "external_proxies")
    :param output_file: åˆå¹¶åè¾“å‡ºçš„æ–‡ä»¶è·¯å¾„ (ä¾‹å¦‚: "merged-proxies.yaml")
    :param name_filter: ä»£ç†åç§°è¿‡æ»¤å™¨, å¯é€‰å€¼ä¸º 'hk', 'us' ç­‰, æˆ– None (ä¸è¿‡æ»¤)
    """
    merged_proxies = []
    seen_identifiers = set()

    proxy_files = glob.glob(f"{proxies_dir}/*.*")
    logging.info(f"å‘ç° {len(proxy_files)} ä¸ªä»£ç†æ–‡ä»¶ï¼Œå‡†å¤‡å¼€å§‹å¤„ç†...")

    for file_path in proxy_files:
        try:
            logging.info(f"--- å¼€å§‹å¤„ç†æ–‡ä»¶: {file_path} ---")
            with open(file_path, 'r', encoding="utf-8") as f:
                data = yaml.safe_load(f)

                if not data or 'proxies' not in data:
                    logging.warning(f"æ–‡ä»¶å†…å®¹ä¸ºç©ºæˆ–ç¼ºå°‘ 'proxies' å­—æ®µ: {file_path}")
                    continue

                for proxy in data['proxies']:
                    name = proxy.get('name')
                    server = proxy.get('server')
                    port = proxy.get('port')
                    proxy_type = proxy.get('type')
                    identifier = (server, proxy_type, port)

                    # --- è¿‡æ»¤é€»è¾‘ ---
                    # 1. æ£€æŸ¥å…³é”®ä¿¡æ¯æ˜¯å¦å®Œæ•´
                    if not all(identifier):
                        logging.warning(f"æ’é™¤ä¿¡æ¯ä¸å®Œæ•´çš„ä»£ç†: {name}")
                        continue

                    # 2. æ£€æŸ¥æ˜¯å¦ä¸ºé‡å¤èŠ‚ç‚¹
                    if identifier in seen_identifiers:
                        # logging.info(f"æ’é™¤é‡å¤ä»£ç†: {name} | {server}:{port}")
                        continue
                    
                    # 3. (æœ€é«˜ä¼˜å…ˆçº§) æ£€æŸ¥æ˜¯å¦åŒ…å«é»‘åå•å…³é”®è¯
                    if any(keyword in name for keyword in BLACKLIST_KEYWORDS):
                        logging.info(f"æ’é™¤å«é»‘åå•å…³é”®è¯çš„ä»£ç†: {name}")
                        continue

                    # 4. æ’é™¤ç‰¹å®šç±»å‹çš„ä¸å®‰å…¨ä»£ç†
                    if proxy_type == 'ss' and proxy.get('cipher', '').lower() == 'ss':
                        logging.info(f"æ’é™¤ä¸å®‰å…¨çš„ SS ä»£ç†: {name}")
                        continue

                    # 5. (ä»…åœ°åŒºç‰ˆæœ¬) æ ¹æ®åç§°ç™½åå•è¿›è¡Œè¿‡æ»¤
                    if name_filter:
                        if name_filter in FILTER_PATTERNS:
                            if not FILTER_PATTERNS[name_filter].search(name):
                                continue # é™é»˜æ’é™¤ä¸åŒ¹é…åœ°åŒºçš„èŠ‚ç‚¹
                        else:
                            logging.error(f"æœªçŸ¥çš„è¿‡æ»¤å™¨: {name_filter}")
                            return
                    
                    # --- æ·»åŠ ä»£ç† ---
                    seen_identifiers.add(identifier)
                    merged_proxies.append(proxy)
                    # filter_msg = f"({name_filter}) " if name_filter else ""
                    # logging.info(f"æ·»åŠ æ–°ä»£ç† {filter_msg}: {name} | ç±»å‹: {proxy_type} | æœåŠ¡å™¨: {server}:{port}")

        except Exception as e:
            logging.error(f"å¤„ç†æ–‡ä»¶ {file_path} æ—¶å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}", exc_info=True)

    logging.info(f"--- æ‰€æœ‰æ–‡ä»¶å¤„ç†å®Œæˆ ---")
    logging.info(f"æ€»å…±åˆå¹¶äº† {len(merged_proxies)} ä¸ªå”¯ä¸€çš„ä»£ç†ã€‚")

    try:
        with open(output_file, 'w', encoding="utf-8") as f:
            yaml.dump({'proxies': merged_proxies}, f, default_flow_style=False, allow_unicode=True)
        logging.info(f"æˆåŠŸå°†åˆå¹¶åçš„ä»£ç†å†™å…¥åˆ°: {output_file}")
    except IOError as e:
        logging.error(f"å†™å…¥æ–‡ä»¶ {output_file} å¤±è´¥: {e}")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="åˆå¹¶ Clash ä»£ç†é…ç½®æ–‡ä»¶ï¼Œæ”¯æŒæŒ‰åœ°åŒºè¿‡æ»¤å’Œå»é‡ã€‚"
    )
    parser.add_argument(
        '--proxies-dir',
        type=str,
        default='external_proxies',
        help='å­˜æ”¾ä»£ç†é…ç½®æ–‡ä»¶çš„ç›®å½•è·¯å¾„'
    )
    parser.add_argument(
        '--output',
        type=str,
        required=True,
        help='åˆå¹¶åè¾“å‡ºçš„æ–‡ä»¶è·¯å¾„ (ä¾‹å¦‚: merged-proxies.yaml)'
    )
    parser.add_argument(
        '--filter',
        type=str,
        choices=list(FILTER_PATTERNS.keys()),
        help="æ ¹æ®åœ°åŒºå…³é”®è¯è¿‡æ»¤ä»£ç†åç§°"
    )

    args = parser.parse_args()

    logging.info(f"å¼€å§‹æ‰§è¡Œåˆå¹¶ä»»åŠ¡: filter='{args.filter}', output='{args.output}'")
    merge_proxies(args.proxies_dir, args.output, args.filter)
    logging.info("ä»»åŠ¡æ‰§è¡Œå®Œæ¯•ã€‚")