
# -*- coding: utf-8 -*-
import yaml
import logging
import subprocess
import sys
import os
from copy import deepcopy

# --- é…ç½®æ—¥å¿—è®°å½• ---
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[logging.StreamHandler()]
)

# --- å…¨å±€å¸¸é‡å®šä¹‰ ---
GENERIC_PROXY_GROUP_NAMES = {
    "é€‰æ‹©ä»£ç†", "è‡ªåŠ¨ä¼˜é€‰", "è´Ÿè½½å‡è¡¡", "æ‰‹åŠ¨é€‰æ‹©", "DIRECT", "REJECT",
    "Apple", "Telegram", "YouTube", "Netflix", "Disney+", "Bilibili", "OpenAI",
}

def is_region_specific_group(group_name):
    """åˆ¤æ–­ä¸€ä¸ªä»£ç†ç»„åç§°æ˜¯å¦æ˜¯åœ°åŒºç‰¹æœ‰çš„ã€‚"""
    region_keywords = ["é¦™æ¸¯", "æ—¥æœ¬", "ç¾å›½", "æ–°åŠ å¡", "è‹±å›½", "å°æ¹¾", "éŸ©å›½"]
    return any(keyword in group_name for keyword in region_keywords)

def run_merge_command(filter_code, output_file):
    """è°ƒç”¨ merge_proxies.py è„šæœ¬æ¥ç”Ÿæˆä¸´æ—¶çš„èŠ‚ç‚¹æ•°æ®æ–‡ä»¶ã€‚"""
    command = [
        sys.executable, # ä½¿ç”¨å½“å‰ Python è§£é‡Šå™¨
        "scripts/merge_proxies.py",
        "--output", output_file
    ]
    if filter_code:
        command.extend(["--filter", filter_code])
    
    logging.info(f"æ‰§è¡Œåˆå¹¶å‘½ä»¤: {' '.join(command)}")
    try:
        subprocess.run(command, check=True, capture_output=True, text=True)
        logging.info(f"æˆåŠŸç”Ÿæˆæ•°æ®æ–‡ä»¶: {output_file}")
    except subprocess.CalledProcessError as e:
        logging.error(f"åˆå¹¶èŠ‚ç‚¹å¤±è´¥ (filter: {filter_code}):\n{e.stderr}")
        raise

def generate_config(base_config, proxies_path, output_path, is_region_specific=False):
    """æ ¹æ®åŸºç¡€é…ç½®ã€ä»£ç†åˆ—è¡¨å’Œç‰ˆæœ¬ç±»å‹ç”Ÿæˆæœ€ç»ˆçš„ Clash é…ç½®æ–‡ä»¶ã€‚"""
    try:
        config = deepcopy(base_config)
        if is_region_specific:
            logging.info(f"ä¸ºåœ°åŒºç‰ˆæœ¬ {output_path} æ‰§è¡Œä»£ç†ç»„è£å‰ª...")
            original_groups = config.get('proxy-groups', [])
            kept_groups = []
            for group in original_groups:
                group_name = group.get('name')
                if group_name in GENERIC_PROXY_GROUP_NAMES or not is_region_specific_group(group_name):
                    kept_groups.append(group)
                else:
                    logging.info(f"  -> è£å‰ªåœ°åŒºç‰¹å®šç»„: {group_name}")
            kept_group_names = {group.get('name') for group in kept_groups}
            for group in kept_groups:
                if 'proxies' in group:
                    original_proxies = group['proxies']
                    group['proxies'] = [p for p in original_proxies if p in kept_group_names or p in GENERIC_PROXY_GROUP_NAMES]
                    if len(original_proxies) != len(group['proxies']):
                        logging.info(f"  -> æ¸…ç†ç»„ '{group['name']}' çš„å†…éƒ¨å¼•ç”¨: {original_proxies} -> {group['proxies']}")
            config['proxy-groups'] = kept_groups
            logging.info("ä»£ç†ç»„è£å‰ªå®Œæˆã€‚")

        with open(proxies_path, 'r', encoding="utf-8") as f:
            proxies_data = yaml.safe_load(f)
        config['proxies'] = proxies_data.get('proxies', [])
        logging.info(f"æˆåŠŸè¯»å–å¹¶åˆå¹¶ {len(config['proxies'])} ä¸ªä»£ç†èŠ‚ç‚¹ã€‚")

        with open(output_path, 'w', encoding="utf-8") as f:
            yaml.dump(config, f, default_flow_style=False, allow_unicode=True)
        logging.info(f"æˆåŠŸç”Ÿæˆé…ç½®æ–‡ä»¶: {output_path}")

    except Exception as e:
        logging.error(f"ç”Ÿæˆé…ç½®æ–‡ä»¶ '{output_path}' æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}", exc_info=True)
        raise

if __name__ == "__main__":
    # --- å”¯ä¸€çš„çœŸç›¸æ¥æº (Single Source of Truth) ---
    configs_to_generate = [
        {"filter": None, "proxies_file": "merged-proxies.yaml", "output": "config/config.yaml", "is_region_specific": False},
        {"filter": "hk", "proxies_file": "merged-proxies_hk.yaml", "output": "config/config_hk.yaml", "is_region_specific": True},
        {"filter": "us", "proxies_file": "merged-proxies_us.yaml", "output": "config/config_us.yaml", "is_region_specific": True},
        {"filter": "jp", "proxies_file": "merged-proxies_jp.yaml", "output": "config/config_jp.yaml", "is_region_specific": True},
        {"filter": "uk", "proxies_file": "merged-proxies_uk.yaml", "output": "config/config_uk.yaml", "is_region_specific": True},
    ]

    # --- æ­¥éª¤1: å‡†å¤‡æ‰€æœ‰éœ€è¦çš„èŠ‚ç‚¹æ•°æ®æ–‡ä»¶ ---
    logging.info("--- å¼€å§‹å‡†å¤‡æ‰€æœ‰éœ€è¦çš„èŠ‚ç‚¹æ•°æ®æ–‡ä»¶ ---")
    for config_info in configs_to_generate:
        run_merge_command(config_info['filter'], config_info['proxies_file'])
    logging.info("--- æ‰€æœ‰èŠ‚ç‚¹æ•°æ®æ–‡ä»¶å‡†å¤‡å°±ç»ª ---")

    # --- æ­¥éª¤2: åŠ è½½å”¯ä¸€çš„â€œæ¯ç‰ˆâ€æ¨¡æ¿ ---
    base_template_path = "config-template.yaml"
    logging.info(f"æ­£åœ¨ä»å”¯ä¸€çš„æºæ¨¡æ¿ {base_template_path} åŠ è½½åŸºç¡€é…ç½®...")
    try:
        with open(base_template_path, 'r', encoding="utf-8") as f:
            base_config_data = yaml.safe_load(f)
        if not base_config_data:
            raise ValueError("åŸºç¡€é…ç½®æ–‡ä»¶ä¸ºç©ºæˆ–æ ¼å¼é”™è¯¯ã€‚")
    except (FileNotFoundError, ValueError) as e:
        logging.critical(f"æ— æ³•åŠ è½½åŸºç¡€æ¨¡æ¿: {e}", exc_info=True)
        sys.exit(1)

    # --- æ­¥éª¤3: å¾ªç¯ç”Ÿæˆæ‰€æœ‰æœ€ç»ˆçš„é…ç½®æ–‡ä»¶ ---
    logging.info("--- å¼€å§‹ç”Ÿæˆæ‰€æœ‰æœ€ç»ˆé…ç½®æ–‡ä»¶ ---")
    generated_files = []
    for i, config_info in enumerate(configs_to_generate, 1):
        logging.info(f"--- ({i}/{len(configs_to_generate)}) å¼€å§‹ç”Ÿæˆ: {config_info['output']} ---")
        generate_config(
            base_config=base_config_data,
            proxies_path=config_info['proxies_file'],
            output_path=config_info['output'],
            is_region_specific=config_info['is_region_specific']
        )
        generated_files.append(config_info['output'])
    
    # --- æ­¥éª¤4: å°†ç”Ÿæˆçš„æ–‡ä»¶åˆ—è¡¨è¾“å‡ºåˆ° GitHub Actions ---
    # è¿™æ˜¯ä¸€ä¸ªå…³é”®æ­¥éª¤ï¼Œç”¨äºåç»­å·¥ä½œæµçš„è‡ªåŠ¨åŒ–
    if 'GITHUB_OUTPUT' in os.environ:
        logging.info("æ­£åœ¨å°†äº§ç‰©æ¸…å•è¾“å‡ºåˆ° GitHub Actions...")
        with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
            # å°†æ–‡ä»¶åˆ—è¡¨è½¬æ¢æˆä¸€ä¸ªç©ºæ ¼åˆ†éš”çš„å­—ç¬¦ä¸²
            file_list_str = ' '.join(generated_files)
            print(f"generated_files={file_list_str}", file=f)
            logging.info(f"è¾“å‡ºçš„æ¸…å•: {file_list_str}")

    logging.info("ğŸ‰ æ‰€æœ‰ä»»åŠ¡å·²æˆåŠŸå®Œæˆï¼")
